{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Intent Classifier based on toxic comment dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Reference: https://en.wikipedia.org/wiki/Multi-label_classification\n",
    "\n",
    "Goal of this notebook is to create a model which predicts a probability of each type of toxicity for each comment. \n",
    "\n",
    "•\tData Exploration\n",
    "\n",
    "•\tText Preprocessing\n",
    "\n",
    "•\tML pipelines\n",
    "\n",
    "•\tEvaluate Classifier \n",
    "\n",
    "•\tPredictions on Test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import plotly\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer \n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "import numpy as np\n",
    "nltk.download(['wordnet', 'punkt', 'stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Train Dataset\n",
    "toxic_comment_data=pd.read_csv(\"dataset/train.csv\")\n",
    "toxic_comment_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display first few rows\n",
    "toxic_comment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View comment\n",
    "toxic_comment_data[\"comment_text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comment_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test Dataset\n",
    "toxic_comment_test=pd.read_csv(\"dataset/test.csv\")\n",
    "toxic_comment_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display first few rows\n",
    "toxic_comment_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are null values\n",
    "toxic_comment_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of length of comments\n",
    "fig = go.Figure(go.Histogram(\n",
    "       x=toxic_comment_data[\"comment_text\"].apply(lambda x:len(x)),\n",
    "       nbinsx=100,\n",
    "   \n",
    "    )\n",
    "               )\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Distribution of length of comments\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'\n",
    "        \n",
    "        },\n",
    "    xaxis_title=\"Length of comments\",\n",
    "    yaxis_title=\"Count\",)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of comments in each labels\n",
    "toxic_comment_data.iloc[:,2:].sum().to_frame(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Number of comments in each labels\n",
    "fig = go.Figure(go.Bar(\n",
    "       x=toxic_comment_data.iloc[:,2:].sum().index,\n",
    "       y=toxic_comment_data.iloc[:,2:].sum().values,\n",
    "   \n",
    "    )\n",
    "               )\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Count of comments in each label\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'\n",
    "        \n",
    "        },\n",
    "    xaxis_title=\"Labels\",\n",
    "    yaxis_title=\"Count\",)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments which have multiple labels\n",
    "toxic_comment_data.iloc[:,2:].sum(axis=1).value_counts().to_frame(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there comments which have multiple labels??\n",
    "fig = go.Figure(go.Bar(\n",
    "       x=toxic_comment_data.iloc[:,2:].sum(axis=1).value_counts().index,\n",
    "       y=toxic_comment_data.iloc[:,2:].sum(axis=1).value_counts().values,\n",
    "   \n",
    "    )\n",
    "               )\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Count of comments with multiple label\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'\n",
    "        \n",
    "        },\n",
    "    xaxis_title=\"Multiple Labels count\",\n",
    "    yaxis_title=\"Count\",)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most common words in the labels\n",
    "for i in range(len(toxic_comment_data.columns[2:])):\n",
    "    label=toxic_comment_data.columns[i+2]\n",
    "    label_filter=toxic_comment_data[toxic_comment_data[label]==1]\n",
    "    wordcloud = WordCloud (\n",
    "                        background_color = 'white',\n",
    "                        stopwords=STOPWORDS,\n",
    "                        collocations=False,\n",
    "                        width = 1000,\n",
    "                        height = 1000\n",
    "                            ).generate(''.join(label_filter[\"comment_text\"].values))\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,25)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.title(\"Common words in \"+ label)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "\n",
    "    # Normalize text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop = stopwords.words(\"english\")\n",
    "    words = [t for t in words if t not in stop]\n",
    "    \n",
    "    # Lemmatization\n",
    "    s = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Algorithm\n",
    "Pipeline_LR = Pipeline([\n",
    "                ('vect', TfidfVectorizer(tokenizer = text_preprocessing,strip_accents='unicode', analyzer='word', ngram_range=(1,3))),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(C=0.1),n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "# Naive Bayes Algorithm\n",
    "Pipeline_NBC = Pipeline([\n",
    "                ('vect', TfidfVectorizer(tokenizer = text_preprocessing,strip_accents='unicode', analyzer='word', ngram_range=(1,3))),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=None))),\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Train data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comments \n",
    "X=toxic_comment_data[\"comment_text\"]\n",
    "#Labels\n",
    "Y=toxic_comment_data.iloc[:,2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.30, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC for each class\n",
    "Labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "roc_auc_list=[]\n",
    "for label in Labels:\n",
    "    print(\"....\",label,\".....\")\n",
    "    Pipeline_LR.fit(X_train, y_train[label])\n",
    "    prediction = Pipeline_LR.predict_proba(X_test)\n",
    "    preds = prediction[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_test[label], preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc_list.append(roc_auc)\n",
    "    print(f'AUC: {roc_auc:.5f}')      \n",
    "    # Plot ROC_AUC\n",
    "    plt.rcParams[\"figure.figsize\"] = (5,5)    \n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean ROC_AUC for Logistic Regression \n",
    "print(\"Mean AUC Score for Logistic regression is \", np.mean(roc_auc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Naive Bayes\n",
    "roc_auc_list_NB=[]\n",
    "for label in Labels:\n",
    "    print(\"....\",label,\".....\")\n",
    "    Pipeline_NBC.fit(X_train, y_train[label])\n",
    "    prediction = Pipeline_NBC.predict_proba(X_test)\n",
    "    preds = prediction[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_test[label], preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc_list_NB.append(roc_auc)\n",
    "    print(f'AUC: {roc_auc:.5f}')      \n",
    "    # Plot ROC_AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean ROC_AUC for Naive Bayes\n",
    "print(\"Mean AUC Score for Logistic regression is \", np.mean(roc_auc_list_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see from the above AUC score for Logistic regression is better . so We will use logistic regression to predict labels for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC for each class\n",
    "Labels=list(toxic_comment_data.columns)[2:]\n",
    "roc_auc_list=[]\n",
    "for label in Labels:\n",
    "    print(\"....\",label,\".....\")\n",
    "    Pipeline_LR.fit(X,Y[label])\n",
    "    prediction = Pipeline_LR.predict_proba(toxic_comment_test.comment_text)\n",
    "    preds = prediction[:, 1]\n",
    "    exec(\"preds_%s = pd.Series(preds)\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data=pd.DataFrame()\n",
    "result_data[\"id\"]=toxic_comment_test[\"id\"]\n",
    "result_data[\"toxic\"]=preds_toxic\n",
    "result_data[\"severe_toxic\"]=preds_severe_toxic\n",
    "result_data[\"obscene\"]=preds_obscene\n",
    "result_data[\"threat\"]=preds_threat\n",
    "result_data[\"insult\"]=preds_insult\n",
    "result_data[\"identity_hate\"]=preds_identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work to improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Hyperparameter tuning for Logistic Regression model\n",
    "##### 2.Try different methods for feature extraction\n",
    "##### 3.Try ensemble classification methods  \n",
    "##### 4.Use pretrained Deep Learning models \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
